#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""POCç´¢å¼•è„šæœ¬ - å°†YAMLæ–‡ä»¶ç´¢å¼•åˆ°Meilisearch"""

import sys
import yaml
import re
from pathlib import Path
from datetime import datetime
import meilisearch

# Windowsç»ˆç«¯UTF-8æ”¯æŒ
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# æ·»åŠ appç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))
from app.config import settings


def extract_urls(yaml_content: dict) -> tuple[list[str], list[str]]:
    """æå–YAMLä¸­çš„URLè·¯å¾„å’Œå…³é”®è¯
    
    Args:
        yaml_content: è§£æåçš„YAMLå†…å®¹
        
    Returns:
        (urlsåˆ—è¡¨, å…³é”®è¯åˆ—è¡¨)
    """
    url_set = set()
    
    if 'http' in yaml_content and yaml_content['http']:
        for req in yaml_content['http']:
            # æå–pathå­—æ®µ
            if 'path' in req and req['path']:
                paths = req['path'] if isinstance(req['path'], list) else [req['path']]
                for path in paths:
                    if path:
                        clean = re.sub(r'\{\{.*?\}\}', '', path.strip())
                        if clean and clean != '/':
                            url_set.add(clean)
            
            # æå–rawå­—æ®µä¸­çš„URL
            if 'raw' in req and req['raw']:
                for raw in req['raw']:
                    match = re.search(
                        r'(GET|POST|PUT|DELETE|PATCH|HEAD|OPTIONS)\s+(\S+)\s+HTTP', 
                        raw, 
                        re.IGNORECASE
                    )
                    if match:
                        clean = re.sub(r'\{\{.*?\}\}', '', match.group(2)).strip()
                        if clean and clean != '/':
                            url_set.add(clean)
    
    # ç”Ÿæˆå…³é”®è¯
    urls = list(url_set)
    url_keywords = []
    for url in urls:
        base = url.split('?')[0]
        url_keywords.append(base)
        url_keywords.extend([s for s in base.split('/') if s])
    
    return urls, list(set(url_keywords))


def read_yaml_files(folder_path: str) -> list[dict]:
    """è¯»å–å¹¶è§£æYAMLæ–‡ä»¶
    
    Args:
        folder_path: YAMLæ–‡ä»¶å¤¹è·¯å¾„
        
    Returns:
        POCæ–‡æ¡£åˆ—è¡¨
    """
    poc_data = []
    poc_id_set = set()
    duplicates = []
    folder = Path(folder_path)
    
    if not folder.exists():
        print(f"âŒ é”™è¯¯: æ–‡ä»¶å¤¹ä¸å­˜åœ¨ - {folder_path}")
        return poc_data
    
    yaml_files = list(folder.rglob("*.yaml")) + list(folder.rglob("*.yml"))
    print(f"ğŸ“ æ‰¾åˆ° {len(yaml_files)} ä¸ªYAMLæ–‡ä»¶\n")
    
    for idx, file in enumerate(yaml_files, 1):
        try:
            content = file.read_text(encoding='utf-8')
            data = yaml.safe_load(content)
            
            if not data:
                continue
            
            poc_id = data.get('id', '')
            
            # å»é‡æ£€æŸ¥
            if poc_id and poc_id in poc_id_set:
                duplicates.append(file.name)
                print(f"[{idx}/{len(yaml_files)}] â­ï¸  {file.name} - è·³è¿‡ï¼ˆé‡å¤ï¼‰")
                continue
            
            # æå–URL
            urls, keywords = extract_urls(data)
            
            # æ„å»ºæ–‡æ¡£
            doc = {
                'id': poc_id or f"unnamed_{file.stem}",
                'poc_id': poc_id,
                'name': data.get('info', {}).get('name', ''),
                'author': data.get('info', {}).get('author', ''),
                'severity': data.get('info', {}).get('severity', ''),
                'description': data.get('info', {}).get('description', ''),
                'tags': data.get('info', {}).get('tags', ''),
                'urls': urls,
                'url_keywords': keywords,
                'yaml_content': content,
                'metadata': data.get('info', {}).get('metadata', {}),
                'indexed_at': datetime.now().isoformat()
            }
            
            poc_data.append(doc)
            if poc_id:
                poc_id_set.add(poc_id)
            
            print(f"[{idx}/{len(yaml_files)}] âœ… {file.name} - {len(urls)} URLs")
            
        except Exception as e:
            print(f"[{idx}/{len(yaml_files)}] âŒ é”™è¯¯: {file.name} - {e}")
    
    if duplicates:
        print(f"\nâš ï¸  è·³è¿‡ {len(duplicates)} ä¸ªé‡å¤POC")
    
    return poc_data


def upload_to_meilisearch(documents: list[dict]):
    """ä¸Šä¼ æ–‡æ¡£åˆ°Meilisearch
    
    Args:
        documents: POCæ–‡æ¡£åˆ—è¡¨
    """
    if not documents:
        print("\nâš ï¸  æ²¡æœ‰æ–‡æ¡£å¯ä¸Šä¼ ")
        return
    
    try:
        client = meilisearch.Client(settings.meilisearch_url, settings.meilisearch_key)
        index = client.index(settings.index_name)
        
        print(f"\nğŸ“¤ ä¸Šä¼  {len(documents)} ä¸ªæ–‡æ¡£åˆ°ç´¢å¼• '{settings.index_name}'...")
        task = index.add_documents(documents, primary_key='id')
        
        # é…ç½®ç´¢å¼•
        index.update_searchable_attributes([
            'urls', 'url_keywords', 'poc_id', 'name', 'description', 'tags', 'author'
        ])
        index.update_filterable_attributes(['severity', 'author', 'tags', 'poc_id'])
        index.update_sortable_attributes(['indexed_at', 'severity'])
        index.update_ranking_rules([
            'words', 'typo', 'proximity', 'attribute', 'sort', 'exactness'
        ])
        
        task_uid = task.task_uid if hasattr(task, 'task_uid') else 'N/A'
        print(f"âœ… ä¸Šä¼ å®Œæˆï¼ä»»åŠ¡ID: {task_uid}")
        
    except Exception as e:
        print(f"âŒ ä¸Šä¼ å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ“š Nuclei POC ç´¢å¼•å·¥å…·")
    print("=" * 60)
    print(f"ç´¢å¼•åç§°: {settings.index_name}")
    print(f"POCæ–‡ä»¶å¤¹: {settings.poc_folder}\n")
    
    # è¯»å–YAMLæ–‡ä»¶
    poc_data = read_yaml_files(settings.poc_folder)
    
    if not poc_data:
        print("\nâš ï¸  æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„POCæ–‡ä»¶")
        return
    
    total_urls = sum(len(doc['urls']) for doc in poc_data)
    print(f"\nğŸ“Š ç»Ÿè®¡: {len(poc_data)} ä¸ªPOC, {total_urls} ä¸ªURL")
    
    # ä¸Šä¼ åˆ°Meilisearch
    upload_to_meilisearch(poc_data)
    
    print("\n" + "=" * 60)
    print("ğŸ‰ ç´¢å¼•å®Œæˆï¼")
    print("=" * 60)


if __name__ == "__main__":
    main()

